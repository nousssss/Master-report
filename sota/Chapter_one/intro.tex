\section{Introduction}\label{sec:hw-nas:intro}

Designing a \gls{abb:dl} architecture is a time-consuming task that does not always yield optimal results. It requires a significant amount of trial and error, along with mathematical intuition, deep understanding, and expertise, because there is an extremely large design space with endless combinations and possibilities to explore.  For instance, to design a convolutional neural network for a specific task, one must choose the number of layers, the number of channels, the size of convolution kernels, resolution, and types of connections between feature maps.

Another issue is that there is no free lunch~\cite{freelunch}; a well-designed architecture may excel in a particular task with a specific dataset and target hardware platform, yet perform poorly on another. In addition, the optimal combination depends on many constraints, so it is necessary have to define the objectives that matter the most according to the use case. We might want the model to be accurate regardless of how much memory and compute it will use, or we could prioritize efficiency—considering factors such as latency for real-time inference, storage for limited available memory, and energy consumption for edge devices with low energy resources.

In this chapter, we will first define the primitive operations and classic building blocks used in designing neural network architectures. Subsequently, we will introduce a widely adopted technique to address the challenge of manual design—\gls{abb:nas}. We will then present ways to craft efficient neural networks, with a particular focus on \gls{abb:hwnas}.